{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb8dd0a1-bf15-443f-85f6-bd443b06044d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Tema 3: Computación Inteligente (LLM)</h1>\n",
    "    <br/>\n",
    "    <h1>Generación Aumentada por Recuperación (RAG)</h1>\n",
    "    <br/>\n",
    "    <h5>Prof. Wladimir Rodríguez</h5>\n",
    "    <h5>wladimir@ula.ve</h5>\n",
    "    <h5>Departamento de Computación</h5>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db72353d-e284-4255-8426-9804ab528d12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Qué son los sistemas RAG?\n",
    "\n",
    "- Combinación de recuperación de información y generación de texto\n",
    "- Desarrollados para superar limitaciones de los LLM tradicionales\n",
    "- Permiten a los modelos acceder a conocimiento externo en tiempo real\n",
    "- Arquitectura híbrida que separa almacenamiento de conocimiento y razonamiento\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfa08fd-1dde-4eb8-af95-53bacd5d3171",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## El problema que resuelven\n",
    "\n",
    "- **Conocimiento limitado**: Los LLM solo conocen lo que aprendieron en entrenamiento\n",
    "- **Alucinaciones**: Generación de información incorrecta pero plausible\n",
    "- **Ventanas de contexto finitas**: Limitación en la cantidad de texto procesable\n",
    "- **Opacidad**: Fuentes de información no verificables ni citables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1873012-ebeb-4e28-9cd2-8ec057897116",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Arquitectura básica de RAG\n",
    "\n",
    "![Arquitectura RAG](../Figuras/RAG-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28cb190-9538-49a5-a5e9-1e80734ba35c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Cómo funciona un sistema RAG?\n",
    "\n",
    "1. Usuario envía una consulta\n",
    "2. Sistema convierte consulta en vector (embedding)\n",
    "3. Búsqueda de información similar en base de conocimiento\n",
    "4. Recuperación de fragmentos relevantes\n",
    "5. Modelo recibe consulta original + contexto recuperado\n",
    "6. Generación de respuesta fundamentada\n",
    "7. Entrega al usuario con posibles citaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074240ed-60be-4d7a-971e-0b50ffa11024",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ventajas de los sistemas RAG\n",
    "\n",
    "- **Información actualizada**: Acceso a datos recientes\n",
    "- **Reducción de alucinaciones**: Respuestas basadas en fuentes concretas\n",
    "- **Transparencia**: Capacidad de citar fuentes\n",
    "- **Personalización**: Adaptable a dominios específicos\n",
    "- **Eficiencia**: No requiere reentrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b6750a-a382-4eb4-91fd-2ef42aca32aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Componentes detallados: Base de conocimiento\n",
    "\n",
    "- **Tipos de información almacenada**:\n",
    "  - Documentos textuales (artículos, manuales, libros)\n",
    "  - Datos estructurados (bases de datos)\n",
    "  - Código fuente y documentación técnica\n",
    "  - Contenido web procesado\n",
    "\n",
    "- **Indexación y procesamiento**:\n",
    "  - División en fragmentos (\"chunking\")\n",
    "  - Generación de metadatos\n",
    "  - Creación de embeddings para búsqueda semántica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1beb668-5ac8-4587-a0ba-6c64db11b5b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Componentes detallados: Sistema de recuperación\n",
    "\n",
    "- **Métodos de búsqueda**:\n",
    "  - Similitud coseno entre vectores\n",
    "  - Algoritmos híbridos (BM25 + semántica)\n",
    "  - Búsqueda densa de pasajes (DPR)\n",
    "\n",
    "- **Técnicas avanzadas**:\n",
    "  - Reordenamiento (re-ranking)\n",
    "  - Expansión de consultas\n",
    "  - Filtrado por metadatos (tiempo, categoría, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295cef25-0e47-4c90-8d16-5779d045b91e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Componentes detallados: Modelo generativo\n",
    "\n",
    "- **Preparación de prompts**:\n",
    "  - Estructuración de la consulta\n",
    "  - Incorporación del contexto recuperado\n",
    "  - Instrucciones específicas para el modelo\n",
    "\n",
    "- **Generación optimizada**:\n",
    "  - Verificación interna de consistencia\n",
    "  - Detección de contradicciones\n",
    "  - Citación de fuentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c365f0bf-859d-4f64-b686-d05e3771f300",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RAG vs. Fine-tuning\n",
    "\n",
    "|                    | Fine-tuning                      | RAG                                |\n",
    "|--------------------|----------------------------------|-----------------------------------|\n",
    "| **Conocimiento**   | Integrado en pesos del modelo    | Separado y accesible              |\n",
    "| **Actualización**  | Requiere reentrenamiento         | Actualización simple de base      |\n",
    "| **Transparencia**  | Limitada (caja negra)            | Alta (fuentes citables)           |\n",
    "| **Recursos**       | Intensivo en computación         | Intensivo en almacenamiento       |\n",
    "| **Personalización**| Adaptación profunda al dominio   | Flexibilidad en fuentes           |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c57fd3-02e2-42c9-8608-3a861350cabd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aplicaciones prácticas\n",
    "\n",
    "- **Empresariales**:\n",
    "  - Sistemas de soporte técnico y atención al cliente\n",
    "  - Asistentes de investigación legal y financiera\n",
    "  - Gestión interna de conocimiento corporativo\n",
    "\n",
    "- **Educativas**:\n",
    "  - Tutores personalizados con acceso a material específico\n",
    "  - Sistemas de respuesta a preguntas académicas\n",
    "  - Asistentes de investigación científica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a81a8e-57aa-4b8f-93b5-f62f0aaef7af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aplicaciones prácticas (cont.)\n",
    "\n",
    "- **Salud**:\n",
    "  - Asistentes médicos con acceso a literatura científica\n",
    "  - Sistemas de apoyo al diagnóstico\n",
    "  - Análisis de historiales clínicos\n",
    "\n",
    "- **Software**:\n",
    "  - Asistentes de programación con acceso a documentación\n",
    "  - Sistemas de gestión de código y documentación\n",
    "  - Automatización de respuestas técnicas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd3d724-ac02-45dd-81bd-7387771dd5ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Desafíos técnicos\n",
    "\n",
    "- **Calidad de recuperación**:\n",
    "  - Relevancia de los resultados\n",
    "  - Manejo de consultas ambiguas\n",
    "\n",
    "- **Integración de contexto**:\n",
    "  - Limitaciones de ventana de contexto\n",
    "  - Priorización de información\n",
    "\n",
    "- **Mantenimiento de conocimiento**:\n",
    "  - Actualización continua\n",
    "  - Detección de información contradictoria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1136d688-778a-4fe9-aad5-d35d575f41ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estrategias de implementación\n",
    "\n",
    "- **Selección de base vectorial**:\n",
    "  - Opciones: Pinecone, Weaviate, Milvus, Chroma, etc.\n",
    "  - Consideraciones de escala y rendimiento\n",
    "\n",
    "- **Modelos de embedding**:\n",
    "  - OpenAI, Cohere, BERT, Sentence-BERT, E5\n",
    "  - Balance entre calidad y eficiencia\n",
    "\n",
    "- **Estrategias de chunking**:\n",
    "  - Por tamaño fijo vs. semántico\n",
    "  - Fragmentos jerárquicos vs. solapados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66ce926-8525-4428-b50c-0f74f994f6db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluación de sistemas RAG\n",
    "\n",
    "- **Métricas clave**:\n",
    "  - Precisión de recuperación\n",
    "  - Calidad de respuestas generadas\n",
    "  - Velocidad de respuesta\n",
    "  - Tasa de citación correcta\n",
    "\n",
    "- **Metodologías**:\n",
    "  - Evaluación humana\n",
    "  - Benchmarks automatizados\n",
    "  - Pruebas A/B con usuarios reales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb3f88e-19f7-49ad-a58d-c3a85382cf68",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tendencias emergentes\n",
    "\n",
    "- **RAG multimodal**:\n",
    "  - Incorporación de imágenes, audio y video\n",
    "  - Razonamiento entre diferentes formatos\n",
    "\n",
    "- **RAG con agentes**:\n",
    "  - Sistemas que deciden autónomamente qué información recuperar\n",
    "  - Razonamiento multi-paso con búsquedas dinámicas\n",
    "\n",
    "- **RAG personalizado**:\n",
    "  - Adaptación a preferencias del usuario\n",
    "  - Mantenimiento de bases de conocimiento individuales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcc9ae0-a318-45f8-aafe-7f0fa1f6dc4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Arquitecturas avanzadas\n",
    "\n",
    "- **RAG reflexivo**:\n",
    "  - Auto-evaluación de resultados\n",
    "  - Refinamiento iterativo de consultas\n",
    "\n",
    "- **RAG híbrido**:\n",
    "  - Combinación con conocimiento paramétrico\n",
    "  - Sistemas con múltiples fuentes de recuperación\n",
    "\n",
    "- **RAG con memoria**:\n",
    "  - Mantenimiento de contexto conversacional\n",
    "  - Aprendizaje de interacciones previas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7cbb02-5922-4a8c-8bf1-0fed04cfd93e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Consideraciones éticas y de gobernanza\n",
    "\n",
    "- **Calidad de la información**:\n",
    "  - Verificación de fuentes\n",
    "  - Prevención de desinformación\n",
    "\n",
    "- **Privacidad**:\n",
    "  - Manejo de datos sensibles\n",
    "  - Controles de acceso\n",
    "\n",
    "- **Transparencia**:\n",
    "  - Explicabilidad de resultados\n",
    "  - Trazabilidad de fuentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c37dab-f2ba-4e0a-bbc4-bca1dd5c1c8e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusiones\n",
    "\n",
    "- Los sistemas RAG representan un paradigma nuevo en IA\n",
    "- Combinan la potencia generativa con precisión informativa\n",
    "- Ofrecen soluciones a problemas fundamentales de los LLM\n",
    "- Permiten personalización sin reentrenamiento\n",
    "- Evolución continua hacia sistemas más sofisticados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f515af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Importar los paquetes necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c70ac69-e463-4cfe-8a6f-8602561a241c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38de5c25",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Registrar Eventos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3662fd1a-44e9-446d-a388-1ffbd8d34891",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Configurar registro de eventos (logging)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8669e86",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Definir Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9728b972",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Constantes\n",
    "CAMINO_DOCUMENTOS = \"../datos/IAG.pdf\"\n",
    "NOMBRE_MODELO = \"llama3.2\"\n",
    "MODELO_VECTORIZACION = \"nomic-embed-text\"\n",
    "NOMBRE_ALMACENAMIENTO_VECTOR = \"simple-rag\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5340b62b-fbaf-4405-93fc-07d1bc9c45ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Función para leer archivos PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05809457-cd30-4189-bd65-59828c4489e3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def leer_pdf(camino_documentos):\n",
    "    \"\"\"cargar documentos.\"\"\"\n",
    "    if os.path.exists(camino_documentos):\n",
    "        cargador = UnstructuredPDFLoader(file_path=camino_documentos)\n",
    "        data = cargador.load()\n",
    "        logging.info(\"PDF cargado exitosamente.\")\n",
    "        return data\n",
    "    else:\n",
    "        logging.error(f\"archivo PDF no encontrado: {camino_documentos}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052bf2c1-6a26-4ab5-a043-b98a780f51eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Función para generar trozos de los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ba40100-f762-44f6-935c-7852e1017c09",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def dividir_documentos(documentos):\n",
    "    \"\"\"Dividir documentos en trozos pequeños.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=300)\n",
    "    trozos = text_splitter.split_documents(documentos)\n",
    "    logging.info(\"Documentos divididos en trozos.\")\n",
    "    return trozos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee5034d-c002-43d7-ac25-313f446d84ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Función para crear base de datos de vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1743f1d8-e74c-4d39-88b2-39a003d5b167",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def crear_bd_vector(trozos):\n",
    "    \"\"\"Crear una base de datos de vectores a partir de los trozos del documento.\"\"\"\n",
    "    # Bajar el modelo de vectorizaciónl si no esta disponible\n",
    "    ollama.pull(MODELO_VECTORIZACION)\n",
    "\n",
    "    vector_db = Chroma.from_documents(\n",
    "        documents=trozos,\n",
    "        embedding=OllamaEmbeddings(model=MODELO_VECTORIZACION),\n",
    "        collection_name=NOMBRE_ALMACENAMIENTO_VECTOR,\n",
    "    )\n",
    "    logging.info(\"Vector database created.\")\n",
    "    return vector_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d2ea26-203f-4408-ad14-059805e9afda",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Función para crear el recuperador de fragmentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90522cf0-dfc5-44ae-b8af-e8b71ac33249",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def crear_recuperador(vector_db, llm):\n",
    "    \"\"\"Create un recuperador de multi-consulta.\"\"\"\n",
    "    QUERY_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"\"\"Eres un asistente de modelo de lenguaje de IA. Tu tarea es generar cinco\n",
    "versiones diferentes de la pregunta del usuario dada para recuperar documentos relevantes de\n",
    "una base de datos de vectores. Al generar múltiples perspectivas sobre la pregunta del usuario, tu\n",
    "objetivo es ayudar al usuario a superar algunas de las limitaciones de la búsqueda de similitud \n",
    "basada en la distancia. Proporciona estas preguntas alternativas separadas por nuevas líneas.\n",
    "Pregunta original: {question}\"\"\",\n",
    "    )\n",
    "\n",
    "    recuperador = MultiQueryRetriever.from_llm(\n",
    "        vector_db.as_retriever(), llm, prompt=QUERY_PROMPT\n",
    "    )\n",
    "    logging.info(\"Crear recuperador.\")\n",
    "    return recuperador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ecccfd-c783-4745-ac02-a1b7d2c6e5f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Crear cadena de Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2a1d64e-349f-48cf-8675-944047118615",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def crear_cadena(recuperador, llm):\n",
    "    \"\"\"Crear la cadena\"\"\"\n",
    "    # RAG prompt\n",
    "    template = \"\"\"Responda la pregunta basándose ÚNICAMENTE en el siguiente contexto:\n",
    "{context}\n",
    "Pregunta: {question}\n",
    "\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    cadena = (\n",
    "        {\"context\": recuperador, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    logging.info(\"Cadena creada exitosamente.\")\n",
    "    return cadena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d78c214-69de-4ec2-a529-5af4dd00cdb3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pikepdf._core:pikepdf C++ to Python logger bridge initialized\n",
      "INFO:root:PDF cargado exitosamente.\n",
      "INFO:root:Documentos divididos en trozos.\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/pull \"HTTP/1.1 200 OK\"\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Vector database created.\n",
      "INFO:root:Cadena creada exitosamente.\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta:\n",
      "Según el texto proporcionado, GuIA (Guía de buenas prácticas) es una iniciativa que tiene como objetivo generar un ecosistema para compartir y conocer las mejores prácticas en inteligencia artificial ética y normativa. Su objetivo es crear comunidad alrededor de las buenas prácticas aterrizadas en inteligencia artificial ética y normativa, con tres pilares estratégicos: investigación, divulgación y formación.\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.utils import open_filename\n",
    "\n",
    "# Cargar y procesar documento de PDF\n",
    "data = leer_pdf(CAMINO_DOCUMENTOS)\n",
    "if data:\n",
    "    # Dividir los documentos en trozos\n",
    "    trozos = dividir_documentos(data)\n",
    "    \n",
    "    # Crear la base de datos de vectores\n",
    "    vector_db = crear_bd_vector(trozos)\n",
    "    \n",
    "    # Iniciálizar el modelo de lenguaje\n",
    "    llm = ChatOllama(model=NOMBRE_MODELO)\n",
    "    \n",
    "    # Crear el recuperador multi-consulta\n",
    "    #recuperador = crear_recuperador(vector_db, llm)\n",
    "    \n",
    "    # Crear el recuperador\n",
    "    recuperador = vector_db.as_retriever()\n",
    "    \n",
    "    # Crear la cadena preservandoosla la sintaxis\n",
    "    cadena = crear_cadena(recuperador, llm)\n",
    "    \n",
    "    # Consulta ejemplo \n",
    "    pregunta = \"Que es GuIA\"\n",
    "    \n",
    "    # Obtener la respuesta\n",
    "    respuesta = cadena.invoke(input=pregunta)\n",
    "    print(\"Respuesta:\")\n",
    "    print(respuesta)\n",
    "else:\n",
    "    print('Archivo no encontrado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e12688c3-a7ed-4751-bcbe-35f59f26a6b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta:\n",
      "Las buenas prácticas recomendadas en el documento se pueden resumir de la siguiente manera:\n",
      "\n",
      "1. **Equidad**: Articular marcos que equilibren objetivos y diferentes definiciones de equidad, priorizando la equidad en algunas situaciones hipotéticas comunes.\n",
      "2. **Protección de la privacidad**: Encontrar un enfoque que equilibre adecuadamente la privacidad y la utilidad para la tarea en cuestión. Utilizar técnicas como \"PATE\" (Private Aggregation of Teacher Ensembles) para transferir conocimiento de modelos maestros a modelos estudiantes con una privacidad inmediata.\n",
      "3. **Seguridad**: Sigue los procesos de mejores prácticas establecidos para el software criptográfico y crítico, como:\n",
      " * Uso de enfoques basados en principios y ser demostrables.\n",
      " * Publicación de nuevas ideas revisadas y aprobadas por la comunidad.\n",
      " * Código abierto de componentes de software críticos.\n",
      " * Contratación de expertos para su revisión en todas las etapas de diseño y desarrollo.\n",
      "4. **Investigación sobre la privacidad**: Realizar investigaciones sobre la privacidad con casos de uso, como evaluar conjuntos de datos de entrenamiento para evitar posibles fuentes de sesgo y crear modelos de entrenamiento para eliminar o corregir dichos sesgos.\n",
      "\n",
      "Es importante mencionar que estas buenas prácticas se enfocan en la inteligencia artificial ética y su aplicación en diferentes sectores, como la privacidad, la equidad y la seguridad.\n"
     ]
    }
   ],
   "source": [
    "# Consulta ejemplo \n",
    "pregunta = \"cuales son las buenas prácticas\"\n",
    "    \n",
    "# Obtener la respuesta\n",
    "respuesta = cadena.invoke(input=pregunta)\n",
    "print(\"Respuesta:\")\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "485628cf-6aed-4cf2-a532-a41cb391c63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta:\n",
      "La SEDIA se refiere a la Secretaría de Estado de Digitalización e Inteligencia Artificial.\n"
     ]
    }
   ],
   "source": [
    "# Consulta ejemplo \n",
    "pregunta = \"que es la SEDIA\"\n",
    "    \n",
    "# Obtener la respuesta\n",
    "respuesta = cadena.invoke(input=pregunta)\n",
    "print(\"Respuesta:\")\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed2502a5-c84c-4edc-9a38-2dbbc4a287e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta:\n",
      "Según el contexto proporcionado, un ejemplo de buena práctica en el uso de inteligencia artificial ética es la evaluación de los conjuntos de datos de entrenamiento para evitar posibles fuentes de sesgo. Esto se menciona en el documento como una práctica recomendada para abordar la equidad en la IA.\n"
     ]
    }
   ],
   "source": [
    "# Consulta ejemplo \n",
    "pregunta = \"Ejemplo de buenas practicas\"\n",
    "    \n",
    "# Obtener la respuesta\n",
    "respuesta = cadena.invoke(input=pregunta)\n",
    "print(\"Respuesta:\")\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9dcb7d-353a-4c9b-a122-d3797619e451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
