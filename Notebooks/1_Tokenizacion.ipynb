{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "281aef4b-b5fc-46af-a533-2355b310d622",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Tema 1: Computación Inteligente (LLM)</h1>\n",
    "    <br/>\n",
    "    <h1>Tokenización</h1>\n",
    "    <br/>\n",
    "    <h5>Prof. Wladimir Rodríguez</h5>\n",
    "    <h5>wladimir@ula.ve</h5>\n",
    "    <h5>Departamento de Computación</h5>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332365ec-2d0a-4de3-8743-5b17176e5d8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Qué es un Tokenizador?\n",
    "\n",
    "- **Definición**: Un tokenizador es un componente esencial en el procesamiento del lenguaje natural (PNL) que:\n",
    "\n",
    "    - Divide el texto de entrada en unidades más pequeñas llamadas \"tokens\".\n",
    "\n",
    "    - Estos tokens pueden ser:\n",
    "\n",
    "        - Palabras\n",
    "\n",
    "        - Subpalabras (fragmentos de palabras)\n",
    "\n",
    "        - Caracteres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b34b66-346d-408d-9766-6b4e4ee57db7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## La Importancia de la Tokenización\n",
    "\n",
    "- **Representación Numérica**: Los LLMs son modelos numéricos. Necesitan convertir el texto en números (IDs de tokens) para poder procesarlo. Sin tokenización, no hay entrada para el modelo.\n",
    "\n",
    "- **Definición del Vocabulario**: El tokenizador define el vocabulario del modelo: el conjunto de tokens que el modelo \"conoce\". Esto afecta directamente a la capacidad del modelo para representar el lenguaje.\n",
    "\n",
    "- **Rendimiento**: La eficiencia del tokenizador impacta la velocidad de entrenamiento e inferencia del LLM. Un tokenizador lento puede convertirse en un cuello de botella.\n",
    "\n",
    "- **Comprensión**: La forma en que se tokeniza el texto influye en la capacidad del modelo para comprender el significado. Un buen tokenizador ayuda al modelo a capturar las relaciones semánticas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f02d5d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenización Basada en Palabras\n",
    "\n",
    "**Ejemplo:**\n",
    "\n",
    "    - Texto: \"¡Hola mundo! ¿Cómo estás?\"\n",
    "\n",
    "    - Tokens (Basado en Espacios): [\"¡Hola\", \"mundo!\", \"¿Cómo\", \"estás?\"]\n",
    "\n",
    "**Problemas:**\n",
    "\n",
    "   - *Puntuación*: La puntuación está pegada a las palabras, dificultando el aprendizaje de relaciones sintácticas y semánticas.\n",
    "\n",
    "   - *Contracciones*: \n",
    "        - She's (Ella es/está): Esta es una contracción de \"She is\" o \"She has\".\n",
    "        - isn't (no es/está): Esta es una contracción de \"is not\".\n",
    "\n",
    "   - *Idiomas*: ¿Cómo manejar idiomas sin espacios entre palabras (chino, japonés)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ea76a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cargar la novela Doña Barbara \n",
    "\n",
    "Cargar el texto sin formato con el que queremos trabajar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33ef4056",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de caracteres: 596622\n",
      "Doña Bárbara\n",
      "(Caracas: Editorial Araluce, 1929, 480 págs.)\n",
      "\n",
      "\n",
      "\n",
      "PRIMERA PARTE\n",
      "\n",
      "\n",
      "I\n",
      "¿Con quién vamos?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"../Datos/DonaBarbara.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    texto_sin_formato = f.read()\n",
    "    \n",
    "print(\"Número total de caracteres:\", len(texto_sin_formato))\n",
    "print(texto_sin_formato[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dce204",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- El objetivo es tokenizar y vectorizar (embed) este texto para un LLM\n",
    "- Desarrollemos un tokenizador simple basado en un texto de muestra simple que luego podamos aplicar al texto anterior|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b87919e6-8ea8-46fb-a5ba-45de8ea87ebb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola', ' ', 'mundo.', ' ', 'Esto,', ' ', 'es', ' ', 'una', ' ', 'prueba']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "texto = 'Hola mundo. Esto, es una prueba'\n",
    "resultado = re.split(r'(\\s)', texto)\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533fe138",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "No solo queremos dividir en espacios en blanco, sino también en comas y puntos, así que modifiquemos la expresión regular para hacer eso también."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "832a2810",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola', ' ', 'mundo', '.', '', ' ', 'Esto', ',', '', ' ', 'es', ' ', 'una', ' ', 'prueba']\n"
     ]
    }
   ],
   "source": [
    "resultado = re.split(r'([,.]|\\s)', texto)\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46b42f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Como podemos ver, esto crea cadenas vacías, eliminémoslas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76b509e9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola', 'mundo', '.', 'Esto', ',', 'es', 'una', 'prueba']\n"
     ]
    }
   ],
   "source": [
    "resultado = [item for item in resultado if item.strip()]\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e365b589",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto se ve bastante bien, pero también manejaremos otros tipos de puntuación, como punto y coma, signos de interrogación, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e17e5885-e9f4-4791-b05b-b63c051ee8ad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¡', 'Hola', 'mundo', '!', '.', '¿', 'Es', '-', 'esto', '-', 'una', 'prueba', '?']\n"
     ]
    }
   ],
   "source": [
    "texto = '¡Hola mundo!. ¿Es -esto- una prueba?'\n",
    "resultado = re.split(r'([,.:;¿?_!¡\"()\\']|-|\\s)', texto)\n",
    "resultado = [item.strip() for item in resultado if item.strip()]\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd73e30c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![tokenizado](../Figuras/Tokenizado.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b6510e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ventajas\n",
    "\n",
    "- **Intuición:** Representa unidades de significado relativamente claras. Las palabras generalmente tienen un significado semántico discernible.\n",
    "\n",
    "- **Secuencias más cortas:** En comparación con la tokenización basada en caracteres, genera secuencias más cortas, lo que reduce la carga computacional.\n",
    "\n",
    "- **Fácil de entender e interpretar:** Los tokens son fácilmente comprensibles para los humanos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b025a0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Desventajas\n",
    "\n",
    "- **Tamaño de vocabulario grande:** El número de palabras únicas en un idioma puede ser muy grande, lo que resulta en un vocabulario grande que requiere más memoria y puede hacer que el entrenamiento del modelo sea más difícil.\n",
    "\n",
    "- **Problema de palabras fuera del vocabulario (OOV):** El modelo no puede procesar palabras que no están en su vocabulario. Esto puede ser un problema si el texto contiene palabras raras, nombres propios, errores ortográficos o palabras nuevas.\n",
    "\n",
    "- **Dificultad para manejar inflexiones:** Las diferentes formas de una misma palabra (e.g., \"correr\", \"corriendo\", \"corrió\") se tratan como palabras diferentes, lo que dificulta que el modelo generalice entre ellas.\n",
    "\n",
    "- **Manejo inconsistente de la puntuación:** La forma en que se maneja la puntuación puede variar según el tokenizador, lo que puede afectar la calidad del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baf1bb13",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '-', '.', '?', 'Es', 'Hola', 'esto', 'mundo', 'prueba', 'una', '¡', '¿']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(list(set(resultado))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "890b38dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Doña', 'Bárbara', '(', 'Caracas', ':', 'Editorial', 'Araluce', ',', '1929', ',', '480', 'págs', '.', ')', 'PRIMERA', 'PARTE', 'I', '¿', 'Con', 'quién', 'vamos', '?', 'Un', 'bongo', 'remonta', 'el', 'Arauca', 'bordeando', 'las', 'barrancas']\n"
     ]
    }
   ],
   "source": [
    "preprocesado = re.split(r'([,.:;¿?_!¡\"()\\']|-|\\s)', texto_sin_formato)\n",
    "preprocesado = [item.strip() for item in preprocesado if item.strip()]\n",
    "print(preprocesado[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e73e3e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Calculemos el número total de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "badf5830",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115979\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocesado))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b321cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenización Basada en Caracteres\n",
    "\n",
    "- **Descripción:**\n",
    "\n",
    "    - El enfoque más granular: cada carácter individual en el texto se convierte en un token.\n",
    "\n",
    "    - El vocabulario consiste en el conjunto de caracteres únicos presentes en los datos de entrenamiento.\n",
    "\n",
    "    - Espacios en blanco, puntuación y otros símbolos se tratan como caracteres regulares.\n",
    "\n",
    "- **Ejemplo:**\n",
    "\n",
    "    - Texto: \"Hola mundo!\"\n",
    "\n",
    "    - Tokens: [\"H\", \"o\", \"l\", \"a\", \" \", \"m\", \"u\", \"n\", \"d\", \"o\", \"!\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b1ff0b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ventajas\n",
    "\n",
    "- **Tamaño de vocabulario extremadamente pequeño**: Ideal para idiomas con conjuntos de caracteres grandes o cuando se busca minimizar la huella de memoria del modelo. El tamaño del vocabulario es limitado al conjunto de carácteres usados\n",
    "\n",
    "- **Robustez frente a palabras fuera del vocabulario (OOV):** No hay problema de OOV, ya que cualquier texto puede ser representado como una secuencia de caracteres conocidos.\n",
    "\n",
    "- **Simple de implementar:** La lógica de tokenización es trivial.\n",
    "\n",
    "- **Útil para manejar errores ortográficos:** Modelos basados en caracteres pueden ser más robustos ante errores ortográficos o variaciones en la escritura."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab978fe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Desventajas\n",
    "\n",
    "- **Secuencias largas:** Las oraciones se convierten en secuencias muy largas de tokens, lo que aumenta la carga computacional, especialmente para modelos basados en Transformers, donde la atención se calcula sobre todas las parejas de tokens.\n",
    "\n",
    "- **Falta de significado a nivel de token:** Los caracteres individuales tienen poco significado semántico por sí mismos. El modelo debe aprender relaciones complejas entre caracteres para comprender el texto.\n",
    "\n",
    "- **Dificultad para capturar dependencias a largo alcance:** Debido a las secuencias largas, puede ser difícil para el modelo aprender dependencias entre palabras que están separadas por muchos caracteres.\n",
    "\n",
    "- **Ineficiencia en la representación:** Se requiere más \"esfuerzo\" para el modelo aprender el significado en comparación con la tokenización a nivel de palabra o subpalabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1fc8b77",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !()*,-.01234589:;?ABCDEFGHIJLMNOPQRSTUVXYZabcdefghijklmnopqrstuvwxyz¡¿ÁÉÑÓÚáéíñóúü—’“”…\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "caracteres = sorted(list(set(texto_sin_formato)))\n",
    "tamaño_vocabulario = len(caracteres)\n",
    "print(''.join(caracteres))\n",
    "print(tamaño_vocabulario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e0ced4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenización Basada en Subpalabras:\n",
    "\n",
    "- **Motivación:**\n",
    "\n",
    "    - Superar las limitaciones de la tokenización basada en palabras y caracteres.\n",
    "\n",
    "    - Reducir el tamaño del vocabulario en comparación con la tokenización basada en palabras.\n",
    "\n",
    "    - Manejar palabras fuera del vocabulario (OOV) de forma más eficaz.\n",
    "\n",
    "    - Capturar la estructura morfológica de las palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9e05ac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Descripción:**\n",
    "\n",
    "    - Divide las palabras en unidades más pequeñas llamadas \"subpalabras\".\n",
    "\n",
    "    - Las subpalabras pueden ser morfemas (unidades significativas más pequeñas), fragmentos de palabras o caracteres individuales.\n",
    "\n",
    "    - El vocabulario consiste en un conjunto de subpalabras que se aprenden a partir de los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae36299",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Algoritmos Comunes de Tokenización de Subpalabras\n",
    "\n",
    "- **Byte Pair Encoding (BPE):**\n",
    "\n",
    "    - *Proceso*:\n",
    "        1. Comienza con un vocabulario que contiene todos los caracteres individuales.\n",
    "\n",
    "        2. Iterativamente, identifica el par de tokens más frecuente en los datos de entrenamiento y los fusiona en un nuevo token.\n",
    "\n",
    "        3. Repite el paso 2 hasta que se alcance el tamaño de vocabulario deseado.\n",
    "\n",
    "    - *Ejemplo*:\n",
    "\n",
    "        - Datos: \"low\", \"lower\", \"lowest\"\n",
    "\n",
    "        - BPE podría fusionar \"l\" y \"o\" -> \"lo\", luego \"lo\" y \"w\" -> \"low\", etc.\n",
    "\n",
    "    - *Ventajas*: Sencillo de implementar, eficaz para reducir el tamaño del vocabulario.\n",
    "\n",
    "    - *Desventajas*: Puede crear subpalabras que no tienen un significado claro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d73af24",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **WordPiece:**\n",
    "\n",
    "    - *Proceso*:\n",
    "\n",
    "        1. Comienza con un vocabulario que contiene todos los caracteres individuales.\n",
    "\n",
    "        2. Iterativamente, identifica el par de tokens que maximiza la probabilidad de los datos al ser fusionados en un nuevo token. Usa un modelo de lenguaje para evaluar la probabilidad.\n",
    "\n",
    "        3. Repite el paso 2 hasta que se alcance el tamaño de vocabulario deseado.\n",
    "\n",
    "    - *Diferencia clave con BPE*: En lugar de solo la frecuencia, WordPiece usa una métrica de verosimilitud basada en el modelo de lenguaje.\n",
    "\n",
    "    - *Ventajas*: Similar a BPE, pero con una base probabilística más sólida.\n",
    "\n",
    "    - *Desventajas*: Similar a BPE.\n",
    "\n",
    "    - *Usado por*: BERT, DistilBERT, MobileBERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01517757",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Unigram Language Model:**\n",
    "\n",
    "    - Proceso:\n",
    "\n",
    "        1. Comienza con un vocabulario grande (e.g., todas las palabras y caracteres).\n",
    "\n",
    "        2. Asigna una probabilidad a cada token en el vocabulario.\n",
    "\n",
    "        3. Iterativamente, elimina los tokens que menos contribuyen a la probabilidad del corpus, hasta alcanzar el tamaño de vocabulario deseado.\n",
    "\n",
    "        4. Para tokenizar, se busca la segmentación del texto en tokens que maximice la probabilidad de la secuencia.\n",
    "\n",
    "    - *Ventajas*: Permite múltiples segmentaciones, útil para tareas como la segmentación de palabras en chino.\n",
    "\n",
    "    - *Desventajas*: Más complejo de implementar que BPE y WordPiece.\n",
    "\n",
    "    - *Usado por*: SentencePiece (puede usar Unigram)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f39461",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **SentencePiece:**\n",
    "\n",
    "    - *Características*:\n",
    "\n",
    "        - Es un tokenizador independiente del idioma que trata el texto como una secuencia de caracteres Unicode.\n",
    "\n",
    "        - Maneja los espacios en blanco como caracteres regulares, lo que evita problemas con la tokenización de espacios en blanco.\n",
    "\n",
    "        - Proporciona una interfaz unificada para BPE, WordPiece y Unigram.\n",
    "\n",
    "    - *Ventajas*: Fácil de usar, versátil, ideal para modelos multilingües.\n",
    "\n",
    "    - *Desventajas*: Puede ser un poco más lento que otros tokenizadores.\n",
    "\n",
    "    - *Usado por*: T5, ALBERT, Marian, muchos modelos modernos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3abe7a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ventajas\n",
    "\n",
    "   - **Vocabulario moderado:** El tamaño del vocabulario es mucho menor que el de la tokenización basada en palabras, pero mayor que el de la tokenización basada en caracteres.\n",
    "\n",
    "   - **Manejo eficaz de OOV:** Puede representar palabras desconocidas como combinaciones de subpalabras conocidas.\n",
    "\n",
    "   - **Captura de la estructura morfológica:** Puede aprender a representar morfemas y otros fragmentos de palabras que tienen significado.\n",
    "\n",
    "   - **Generalización mejorada:** Permite que el modelo generalice mejor a palabras nuevas y raras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fd2049",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Desventajas\n",
    "\n",
    "   - **Mayor complejidad:** Más complejo de implementar y entender que la tokenización basada en palabras o caracteres.\n",
    "\n",
    "   - **Tokens menos intuitivos:** Los tokens de subpalabras pueden ser menos intuitivos para los humanos que las palabras completas.\n",
    "\n",
    "   - **Posible pérdida de información:** En algunos casos, la división en subpalabras puede resultar en la pérdida de información semántica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5578690",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Crear un tokenizador\n",
    "\n",
    "- ### Convertir tokens en identificadores de token\n",
    "\n",
    "     - A continuación, convertimos los tokens de texto en identificadores de token que podemos procesar mediante capas de vectorización (embedding) más tarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4dd4f97",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14724\n"
     ]
    }
   ],
   "source": [
    "todas_las_palabras = sorted(set(preprocesado))\n",
    "tamaño_vocabulario = len(todas_las_palabras)\n",
    "\n",
    "print(tamaño_vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9f1932c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "vocabulario = {token:entero for entero,token in enumerate(todas_las_palabras)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece8325",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A continuación se muestran las primeras 50 entradas de este vocabulario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4e0a3e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('(', 1)\n",
      "(')', 2)\n",
      "('*', 3)\n",
      "(',', 4)\n",
      "('-', 5)\n",
      "('.', 6)\n",
      "('1929', 7)\n",
      "('20', 8)\n",
      "('33', 9)\n",
      "('480', 10)\n",
      "('5', 11)\n",
      "('90', 12)\n",
      "(':', 13)\n",
      "(';', 14)\n",
      "('?', 15)\n",
      "('A', 16)\n",
      "('Abajo', 17)\n",
      "('Abandonarla', 18)\n",
      "('Abandonó', 19)\n",
      "('Abeja', 20)\n",
      "('Abran', 21)\n",
      "('Acababa', 22)\n",
      "('Acabar', 23)\n",
      "('Acabarían', 24)\n",
      "('Acabe', 25)\n",
      "('Acabó', 26)\n",
      "('Acariciándolo', 27)\n",
      "('Acaso', 28)\n",
      "('Acción', 29)\n",
      "('Aceptación', 30)\n",
      "('Aceptó', 31)\n",
      "('Achaguas', 32)\n",
      "('Acometido', 33)\n",
      "('Acupe', 34)\n",
      "('Acusación', 35)\n",
      "('Acuérdese', 36)\n",
      "('Acuéstate', 37)\n",
      "('Acábese', 38)\n",
      "('Adelante', 39)\n",
      "('Además', 40)\n",
      "('Adoración', 41)\n",
      "('Advierte', 42)\n",
      "('Afortunadamente', 43)\n",
      "('Afuera', 44)\n",
      "('Agazapados', 45)\n",
      "('Agradable', 46)\n",
      "('Aguaita', 47)\n",
      "('Aguaite', 48)\n",
      "('Aguaiten', 49)\n",
      "('Aguarde', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocabulario.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a41b81",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ahora lo ponemos todo junto en una clase tokenizadora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dc55cb3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class TokenizadorSimpleV1:\n",
    "    def __init__(self, vocabulario):\n",
    "        self.str_to_int = vocabulario\n",
    "        self.int_to_str = {i:s for s,i in vocabulario.items()}\n",
    "    \n",
    "    def encode(self, texto):\n",
    "        preprocesado = re.split(r'([,.:;¿?_!¡\"()\\']|-|\\s)', texto)\n",
    "                                \n",
    "        preprocesado = [\n",
    "            item.strip() for item in preprocesado if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocesado]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        texto = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Reemplazar los espacios antes de las puntuaciones especificadas\n",
    "        texto = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', texto)\n",
    "        return texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b7d2f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- La función de codificación `encode` convierte el texto en identificadores de token\n",
    "- La función de decodificación `decode` convierte los identificadores de token nuevamente en texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e0cfde6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[464, 3097, 8723, 7498, 2820, 9086, 13592, 8512, 14026, 10271, 8920, 4847, 7202, 6]\n"
     ]
    }
   ],
   "source": [
    "tokenizador = TokenizadorSimpleV1(vocabulario)\n",
    "\n",
    "texto = 'Dos bogas lo hacen avanzar mediante una lenta y penosa maniobra de galeotes.'\n",
    "ids = tokenizador.encode(texto)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816dc234",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Podemos decodificar los números enteros y convertirlos en texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a13810b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dos bogas lo hacen avanzar mediante una lenta y penosa maniobra de galeotes.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizador.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d065ffb3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agregar tokens de contexto especiales\n",
    "\n",
    "- Es útil agregar algunos tokens \"especiales\" para palabras desconocidas y para indicar el final de un texto.\n",
    "- Algunos tokenizadores utilizan tokens especiales para ayudar al LLM con contexto adicional\n",
    "- Algunos de estos tokens especiales son\n",
    "    - [BOS] (inicio de secuencia) marca el comienzo del texto\n",
    "    - [EOS] (fin de secuencia) marca donde termina el texto (esto se usa generalmente para concatenar múltiples textos no relacionados, p. ej., dos artículos de Wikipedia diferentes o dos libros diferentes, etc.)\n",
    "    - [PAD] (relleno) si entrenamos LLM con un tamaño de lote mayor a 1 (podemos incluir múltiples textos con diferentes longitudes; con el token de relleno, rellenamos los textos más cortos hasta la longitud más larga para que todos los textos tengan la misma longitud)\n",
    "    - [UNK] para representar palabras que no están incluidas en el vocabulario\n",
    "    \n",
    "    - <|endoftext|> es análogo al token [EOS] mencionado anteriormente\n",
    "- Utilizamos los tokens <|endoftext|> entre dos fuentes de texto independientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04ec328f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hola'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m texto \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHola, ¿te gusta el té? ¿Es esto una prueba?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtokenizador\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexto\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m, in \u001b[0;36mTokenizadorSimpleV1.encode\u001b[0;34m(self, texto)\u001b[0m\n\u001b[1;32m      7\u001b[0m preprocesado \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;¿?_!¡\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|-|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, texto)\n\u001b[1;32m      9\u001b[0m preprocesado \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocesado \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     11\u001b[0m ]\n\u001b[0;32m---> 12\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocesado]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m preprocesado \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;¿?_!¡\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|-|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, texto)\n\u001b[1;32m      9\u001b[0m preprocesado \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocesado \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     11\u001b[0m ]\n\u001b[0;32m---> 12\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocesado]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Hola'"
     ]
    }
   ],
   "source": [
    "texto = \"Hola, ¿te gusta el té? ¿Es esto una prueba?\"\n",
    "tokenizador.encode(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ff7914",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Lo anterior produce un error porque la palabra \"Hola\" no está incluida en el vocabulario.\n",
    "- Para solucionar estos casos, podemos agregar tokens especiales como \"<|unk|>\" al vocabulario para representar palabras desconocidas.\n",
    "- Como ya estamos ampliando el vocabulario, agreguemos otro token llamado \"<|endoftext|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e647be58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "todos_los_tokens = sorted(list(set(preprocesado)))\n",
    "todos_los_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocabulario = {token:entero for entero,token in enumerate(todos_los_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9e771a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14726"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulario.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7310c6f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('“—Yo', 14721)\n",
      "('“—Ése', 14722)\n",
      "('”', 14723)\n",
      "('<|endoftext|>', 14724)\n",
      "('<|unk|>', 14725)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocabulario.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11621307",
   "metadata": {},
   "source": [
    "- También debemos ajustar el tokenizador en consecuencia para que sepa cuándo y cómo usar el nuevo token `<unk>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82b732cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class TokenizadorSimpleV2:\n",
    "    def __init__(self, vocabulario):\n",
    "        self.str_to_int = vocabulario\n",
    "        self.int_to_str = {i:s for s,i in vocabulario.items()}\n",
    "    \n",
    "    def encode(self, texto):\n",
    "        preprocesado = re.split(r'([,.:;¿?_!¡\"()\\']|-|\\s)', texto)\n",
    "                                \n",
    "        preprocesado = [\n",
    "            item.strip() for item in preprocesado if item.strip()\n",
    "        ]\n",
    "        preprocesado = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocesado\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocesado]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        texto = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Reemplazar los espacios antes de las puntuaciones especificadas\n",
    "        texto = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', texto)\n",
    "        return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a735b6f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14725,\n",
       " 4,\n",
       " 14087,\n",
       " 13019,\n",
       " 7411,\n",
       " 5836,\n",
       " 14725,\n",
       " 15,\n",
       " 14087,\n",
       " 536,\n",
       " 6605,\n",
       " 13592,\n",
       " 11054,\n",
       " 15]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizador2 = TokenizadorSimpleV2(vocabulario)\n",
    "texto = \"Hola, ¿te gusta el té? ¿Es esto una prueba?\"\n",
    "tokenizador2.encode(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b44b859",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola, ¿te gusta el té? <|endoftext|> En las soleadas terrazas del palacio.\n"
     ]
    }
   ],
   "source": [
    "texto1 = \"Hola, ¿te gusta el té?\"\n",
    "texto2 = \"En las soleadas terrazas del palacio.\"\n",
    "\n",
    "texto = \" <|endoftext|> \".join((texto1, texto2))\n",
    "\n",
    "print(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "127333ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14725,\n",
       " 4,\n",
       " 14087,\n",
       " 13019,\n",
       " 7411,\n",
       " 5836,\n",
       " 14725,\n",
       " 15,\n",
       " 14724,\n",
       " 509,\n",
       " 8457,\n",
       " 14725,\n",
       " 14725,\n",
       " 4980,\n",
       " 14725,\n",
       " 6]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizador2.encode(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2afd051",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, ¿ te gusta el <|unk|>? <|endoftext|> En las <|unk|> <|unk|> del <|unk|>.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizador2.decode(tokenizador2.encode(texto))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfecabe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenizador usando el algoritmo BytePair\n",
    "\n",
    "- El tokenizador BPE original se puede encontrar aquí: [https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
    "- Utilizamos el tokenizador BPE de la biblioteca de código abierto [tiktoken](https://github.com/openai/tiktoken) de OpenAI, que implementa sus algoritmos centrales en Rust para mejorar el rendimiento computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9246ee7b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de tiktoken: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"Versión de tiktoken:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2db1ebba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tokenizador3 = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d9e60c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39, 5708, 11, 1587, 123, 660, 35253, 64, 1288, 256, 2634, 30, 220, 50256, 2039, 39990, 6195, 38768, 1059, 3247, 292, 1619, 6340, 48711, 13]\n"
     ]
    }
   ],
   "source": [
    "texto1 = \"Hola, ¿te gusta el té?\"\n",
    "texto2 = \"En las soleadas terrazas del palacio.\"\n",
    "\n",
    "texto = \" <|endoftext|> \".join((texto1, texto2))\n",
    "\n",
    "enteros = tokenizador3.encode(texto, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(enteros)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
