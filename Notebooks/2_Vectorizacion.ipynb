{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6213b136-93ab-4d8f-a968-a8378bb791b6",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Tema 1: Computación Inteligente (LLM)</h1>\n",
    "    <br/>\n",
    "    <h1>Vectorización (Embeddings)</h1>\n",
    "    <br/>\n",
    "    <h5>Prof. Wladimir Rodríguez</h5>\n",
    "    <h5>wladimir@ula.ve</h5>\n",
    "    <h5>Departamento de Computación</h5>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1553f720-8cb0-436b-92b3-7ca6f786ccf6",
   "metadata": {},
   "source": [
    "## Motivación - Representaciones\n",
    "- Problema: Representación de palabras en modelos de lenguaje\n",
    "\n",
    "    - One-hot encoding: Representa palabras como vectores binarios.\n",
    "\n",
    "    - Bag of Words (BoW): Representa documentos como conteo de palabras.\n",
    "\n",
    "    - TF-IDF: Mejora BoW ponderando palabras según importancia.\n",
    "\n",
    "- Limitaciones:\n",
    "\n",
    "    - Alta dimensionalidad (matrices dispersas).\n",
    "\n",
    "    - No captura relaciones semánticas entre palabras.\n",
    "\n",
    "    - No considera contexto en el que aparece la palabra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856cbc0e-b4e0-41e3-b965-8d5e41dbf7cf",
   "metadata": {},
   "source": [
    "![One-Hot Encoding](../Figuras/LLM_5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5f5207-676e-42ec-a97b-73d97ad29dfd",
   "metadata": {},
   "source": [
    "## Bolsa de Palabras\n",
    "\n",
    "![Bolsa de Palabras](../Figuras/The-bag-of-words.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc2f2f8-7d1b-4330-ab00-fcdbcd54b6d9",
   "metadata": {},
   "source": [
    "## ¿Qué es la Vectorización (Embeddings)?\n",
    "\n",
    "- Definición:\n",
    "\n",
    "    - Representaciones densas y continuas de palabras en un espacio vectorial.\n",
    "\n",
    "    - Asignan a cada palabra un vector de $n$ dimensiones.\n",
    "\n",
    "    - Aprendidos a partir de grandes corpus de texto.\n",
    "\n",
    "- Beneficios:\n",
    "\n",
    "    - Reducción de dimensionalidad.\n",
    "\n",
    "    - Captura relaciones semánticas y sintácticas.\n",
    "\n",
    "    - Permite comparaciones matemáticas entre palabras.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0692aa0b-f5ac-4581-9925-fb9c58cae6a8",
   "metadata": {},
   "source": [
    "## Relaciones Semánticas y Sintácticas\n",
    "\n",
    "- Ejemplos de embeddings capturando significado:\n",
    "\n",
    "    - \"Rey - Hombre + Mujer = Reina\"\n",
    "\n",
    "    - Palabras similares en el espacio vectorial están cerca (Ejemplo: \"gato\", \"perro\", \"mascota\").\n",
    "\n",
    "    - Relaciones morfológicas: \"correr\" está cerca de \"corriendo\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d55c001-f2ea-495a-8cbe-1a5472892625",
   "metadata": {},
   "source": [
    "## Relaciones Semánticas\n",
    "\n",
    "![Word2Vec](../Figuras/Modelo-de-representacion-Word2Vec.png)\n",
    "\n",
    "![Ejemplo](../Figuras/LLM_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9b011-8087-4d09-8d45-21372fff422e",
   "metadata": {},
   "source": [
    "## ¿Cómo se Entrenan los Embeddings?\n",
    "\n",
    "- Idea principal:\n",
    "\n",
    "    - Aprender representaciones a partir de contextos de palabras en grandes cantidades de texto.\n",
    "\n",
    "    - Redes neuronales entrenadas con tareas como predecir palabras en contexto.\n",
    "\n",
    "- Ejemplo:\n",
    "\n",
    "    - En \"El gato está en la casa\", el modelo aprende que \"gato\" y \"casa\" aparecen en contextos similares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f0746e-6203-48bb-98ee-efb15c786039",
   "metadata": {},
   "source": [
    "## Tipos de Modelos de Embeddings\n",
    "\n",
    "- Embeddings Estáticos (Palabra tiene un único vector fijo):\n",
    "\n",
    "    - Word2Vec (CBOW y Skip-gram)\n",
    "\n",
    "    - GloVe\n",
    "\n",
    "    - FastText\n",
    "\n",
    "- Embeddings Contextuales (El significado de la palabra cambia según el contexto):\n",
    "\n",
    "    - ELMo\n",
    "\n",
    "    - BERT\n",
    "\n",
    "    - GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0944a5-d2f8-47f0-8804-b0531d640763",
   "metadata": {},
   "source": [
    "## Modelos Clásicos de Embeddings\n",
    "\n",
    "- **Word2Vec**\n",
    "\n",
    "  - Dos enfoques principales:\n",
    "\n",
    "    - **CBOW (Continuous Bag of Words)**: Predice la palabra central a partir de su contexto.\n",
    "\n",
    "    - **Skip-gram**: Predice palabras de contexto a partir de una palabra central.\n",
    "\n",
    "  - Basado en redes neuronales simples.\n",
    "\n",
    "  - Captura relaciones semánticas bien estructuradas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fa1194-e4df-42df-a281-cb4196dc2a39",
   "metadata": {},
   "source": [
    "## CBOW (Continuous Bag of Words)\n",
    "\n",
    "![CBOW](../Figuras/LLM_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b9b132-cc71-4fe9-a69d-339fe4c88687",
   "metadata": {},
   "source": [
    "## Skip-gram\n",
    "\n",
    "![Skip-gram](../Figuras/LLM_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee211f8-1abe-4d6b-80b5-bb8dac74c9c9",
   "metadata": {},
   "source": [
    "## Modelos Clásicos de Embeddings\n",
    "\n",
    "- **GloVe**\n",
    "\n",
    "  - Basado en la factorización de matrices de coocurrencia.\n",
    "\n",
    "  - Aprende representaciones a partir de distribuciones globales de palabras.\n",
    "\n",
    "  - Modela relaciones de palabras de manera efectiva.\n",
    "\n",
    "- **FastText**\n",
    "\n",
    "  - Mejora Word2Vec al considerar subpalabras.\n",
    "\n",
    "  - Captura información morfológica.\n",
    "\n",
    "  - Mejora representaciones para palabras raras y derivadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7f1048-d463-49e3-908e-0ae9d805fd65",
   "metadata": {},
   "source": [
    "## Embeddings Contextuales\n",
    "\n",
    "- Diferencia clave respecto a embeddings estáticos\n",
    "\n",
    "  - Una misma palabra puede tener diferentes representaciones según el contexto.\n",
    "\n",
    "  - Modelos más sofisticados permiten capturar significado dinámico.\n",
    "\n",
    "- **ELMo (Embeddings from Language Models)**\n",
    "\n",
    "  - Basado en modelos de redes neuronales recurrentes (LSTM bidireccional).\n",
    "\n",
    "  - Representación dependiente del contexto completo de la oración."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6120600-fec9-4e7b-81b4-01f3d69e10c6",
   "metadata": {},
   "source": [
    "## Embeddings Contextuales\n",
    "\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers)**\n",
    "\n",
    "  - Modelo basado en Transformers.\n",
    "\n",
    "  - Considera contexto anterior y posterior simultáneamente.\n",
    "\n",
    "  - Usa preentrenamiento con \"Máscara de palabras\" (Masked Language Model, MLM).\n",
    "\n",
    "- **GPT (Generative Pre-trained Transformer)**\n",
    "\n",
    "  - Modelo autoregresivo basado en Transformers.\n",
    "\n",
    "  - Entrenado para predecir la siguiente palabra en secuencias de texto.\n",
    "\n",
    "  - Utilizado en tareas de generación de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6356117f-a2f8-466f-a3c5-62587af10857",
   "metadata": {},
   "source": [
    "## Comparación entre Modelos de Embeddings\n",
    "\n",
    "| Modelo | Tipo | Ventaja Principal |\n",
    "|:------:|:-----:|:--------:|\n",
    "| Word2Vec | Estático | Representaciones semánticas claras|\n",
    "| GloVe | Estático | Basado en coocurrencia global |\n",
    "| FastText | Estático | Considera subpalabras |\n",
    "| ELMo | Contextual | Representaciones dependientes del contexto |\n",
    "| BERT | Contextual | Captura contexto bidireccionalmente |\n",
    "| GPT | Contextual | Excelente en generación de texto |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa5ea2b-97bf-4128-913a-3975ebc1d386",
   "metadata": {},
   "source": [
    "## Aplicaciones de los Embeddings\n",
    "\n",
    "- **Clasificación de Texto**\n",
    "\n",
    "  - Spam vs. No spam en correos electrónicos.\n",
    "\n",
    "  - Detección de sentimiento en redes sociales.\n",
    "\n",
    "- **Búsqueda Semántica y Recuperación de Información**\n",
    "\n",
    "  - Motores de búsqueda más precisos (Google, Bing).\n",
    "\n",
    "  - Sistemas de recomendación basados en similitud de palabras y conceptos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845acf4b-e612-4e8d-9bdc-eaaa59e57e81",
   "metadata": {},
   "source": [
    "## Aplicaciones de los Embeddings\n",
    "\n",
    "- **Generación de Texto**\n",
    "\n",
    "  - Chatbots y asistentes virtuales (Siri, Alexa, ChatGPT).\n",
    "\n",
    "  - Composición de artículos y resúmenes automáticos.\n",
    "\n",
    "- **Procesamiento de Texto en Medicina y Finanzas**\n",
    "\n",
    "  - Análisis de historias clínicas para diagnóstico asistido.\n",
    "\n",
    "  - Procesamiento de documentos legales y financieros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75211648-c897-486a-9b77-cd8b7591ba80",
   "metadata": {},
   "source": [
    "## Implementación Práctica en Código\n",
    "\n",
    "### Ejemplo: Uso de Word2Vec en Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e7310ae-846b-4106-b8a2-d5b7fea486ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00856557  0.02826563  0.05401429  0.07052656 -0.05703121  0.0185882\n",
      "  0.06088864 -0.04798051 -0.03107261  0.0679763 ]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Datos de entrenamiento (ejemplo simple)\n",
    "corpus = [[\"gato\", \"perro\", \"animal\"], [\"coche\", \"camión\", \"vehículo\"], [\"computadora\", \"teclado\", \"tecnología\"]]\n",
    "\n",
    "# Entrenar modelo Word2Vec\n",
    "modelo = Word2Vec(sentences=corpus, vector_size=10, window=2, min_count=1, workers=4)\n",
    "\n",
    "# Obtener el embedding de \"gato\"\n",
    "print(modelo.wv[\"gato\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e027522e-17cf-4240-bd89-0bd7a2006f13",
   "metadata": {},
   "source": [
    "### Ejemplo: Uso de Embeddings Preentrenados de BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "177dfcd4-425d-42d3-8df9-98df2b3003d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101, 7570, 2721, 1010, 25989, 999, 102]]\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokens = tokenizer(['hola, mundo!'])\n",
    "print(tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e7a4cc6-c63d-44cf-9d44-4cbab2f874c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0102, -0.0615, -0.0265,  ..., -0.0199, -0.0372, -0.0098],\n",
      "        [-0.0117, -0.0600, -0.0323,  ..., -0.0168, -0.0401, -0.0107],\n",
      "        [-0.0198, -0.0627, -0.0326,  ..., -0.0165, -0.0420, -0.0032],\n",
      "        ...,\n",
      "        [-0.0218, -0.0556, -0.0135,  ..., -0.0043, -0.0151, -0.0249],\n",
      "        [-0.0462, -0.0565, -0.0019,  ...,  0.0157, -0.0139, -0.0095],\n",
      "        [ 0.0015, -0.0821, -0.0160,  ..., -0.0081, -0.0475,  0.0753]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "embedding_layer = model.embeddings.word_embeddings\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c0e2e7-2e74-4356-ba2c-aa00356c9a5b",
   "metadata": {},
   "source": [
    "![Embeddins](../Figuras/TokenEmbedding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33dd1f6d-0928-4f6e-b421-8673b48f37f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hola, mundo! [SEP]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([101, 7570, 2721, 1010, 25989, 999, 102])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
